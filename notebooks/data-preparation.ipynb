{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Preparacion y limpieza de datos\n",
    "\n",
    "Comenzamos importando las librerías de carga de datos y de visualización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego definiremos los tipos de las variables y ademas se cargaran los datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer los datos y establecer los tipos de datos para cada columna basado en la información del pdf\n",
    "dtypes = {'Vlo-I': str, 'Ori-I': str, 'Des-I': str, 'Emp-I': str,\n",
    "          'Vlo-O': str, 'Ori-O': str, 'Des-O': str, 'Emp-O': str,\n",
    "          'DIA': int, 'MES': int, 'AÑO': int, 'DIANOM': str, 'TIPOVUELO': str,\n",
    "          'OPERA': str, 'SIGLAORI': str, 'SIGLADES': str, 'Temporada alta': int,\n",
    "          'Diferencia en minutos': int, 'Atraso menor': int, 'Periodo día': str}\n",
    "\n",
    "df = pd.read_csv('../data/synthetic_features.csv', dtype=dtypes, parse_dates=['Fecha-I', 'Fecha-O'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzamos eliminando la informacion repetida segun la etapa de exploracion de datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Vlo-O', 'Des-O', 'SIGLAORI', 'Ori-O','Ori-I'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora eliminaremos las columnas que no disponemos al ahora de la prediccion, que serian las relacionadas a la operacion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Fecha-O', 'Emp-O', 'DIA', 'MES','AÑO',], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 68205 entries, 0 to 68204\n",
      "Data columns (total 12 columns):\n",
      " #   Column                 Non-Null Count  Dtype         \n",
      "---  ------                 --------------  -----         \n",
      " 0   Fecha-I                68205 non-null  datetime64[ns]\n",
      " 1   Vlo-I                  68205 non-null  object        \n",
      " 2   Des-I                  68205 non-null  object        \n",
      " 3   Emp-I                  68205 non-null  object        \n",
      " 4   DIANOM                 68205 non-null  object        \n",
      " 5   TIPOVUELO              68205 non-null  object        \n",
      " 6   OPERA                  68205 non-null  object        \n",
      " 7   SIGLADES               68205 non-null  object        \n",
      " 8   Temporada alta         68205 non-null  int64         \n",
      " 9   Diferencia en minutos  68205 non-null  int64         \n",
      " 10  Atraso menor           68205 non-null  int64         \n",
      " 11  Periodo día            68205 non-null  object        \n",
      "dtypes: datetime64[ns](1), int64(3), object(8)\n",
      "memory usage: 6.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fecha-I                  53252\n",
       "Vlo-I                      584\n",
       "Des-I                       64\n",
       "Emp-I                       30\n",
       "DIANOM                       7\n",
       "TIPOVUELO                    2\n",
       "OPERA                       23\n",
       "SIGLADES                    62\n",
       "Temporada alta               2\n",
       "Diferencia en minutos      176\n",
       "Atraso menor                 2\n",
       "Periodo día                  3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se menciono en la etapa de exploracion de datos, un atraso menor sera considerado como un atraso de todas maneras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Label'] = (df['Diferencia en minutos'] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos una etiqueta para nuestros datos, ya no necesitaremos la variable diferencia en minutos y el tipo de atraso, ya que esta variable es informacion que no tendremos a la hora de crear predicciones en el mundo real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Diferencia en minutos','Atraso menor'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fecha-I           53252\n",
       "Vlo-I               584\n",
       "Des-I                64\n",
       "Emp-I                30\n",
       "DIANOM                7\n",
       "TIPOVUELO             2\n",
       "OPERA                23\n",
       "SIGLADES             62\n",
       "Temporada alta        2\n",
       "Periodo día           3\n",
       "Label                 2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codificacion de variables categoricas\n",
    "\n",
    "Ahora que tenemos solo los datos de los cuales disponemos a la hora de crear predicciones, podemos comenzar a codificar las variables categoricas. Para esto primero debemos ver que variables son categoricas y si son ordinales o no. <br>\n",
    "\n",
    "- **Categoricas Ordinales**: Son aquellas que tienen un orden logico, por ejemplo, el tipo de atraso, ya que un atraso de 5 minutos es menor que un atraso de 10 minutos. <br>\n",
    "\n",
    "- **Categoricas No Ordinales**: Son aquellas que no tienen un orden logico, por ejemplo, el dia de la semana, ya que no hay un dia de la semana que sea mayor que otro. <br>\n",
    "\n",
    "Con esto en mente comenzamos a clasificar las variables categoricas.\n",
    "\n",
    "- __Vlo-I__: Categorica no ordinal, dado que los codigos de vuelono representan un orden logico. <br>\n",
    "- __Des-I__: Categorica no ordinal, dado que los codigos de  ciudad no representan un orden logico. <br>\n",
    "- __Emp-I__: Categorica no ordinal, dado que los codigos de aerolinea no representan un orden logico. <br>\n",
    "- __TIPOVUELO__: Categorica no ordinal, ya que los valores \"I\" y \"N\" representan tipos de vuelo distintos sin un orden lógico entre ellos. <br>\n",
    "- __OPERA__: Categorica no ordinal, dado que los nombres de aerolínea no representan un orden lógico.\n",
    "- __Temporada alta__: Categorica no ordinal, dado que es solo un indicador del tipo de temporada. <br>\n",
    "- __SIGLADES__: Categorica no ordinal, dado que el nombrede  la  ciudad no representan un orden logico. <br>\n",
    "\n",
    "Las siguientes variables son categorica, pero presentan el problema de una caracteristica ciclica/ordinal. Para analizarlas se puede utilizar tanto una codificacion ordinal como una codificacion ciclica, en este caso se __asumiran ordinales__ . <br>\n",
    "\n",
    "- __Periodo día__: Categorica ordinal. <br>\n",
    "- __DIANOM__: Categorica no ordinal, ya que es una representacion en palabras de un dia. <br>\n",
    "\n",
    "\n",
    "Finalmente, la fecha (Fecha-I), es una variable ciclica, por lo que para representarla como tal se descompondra en mes, dia, hora y minutos. Para luego codificarla como una variable ciclica descomponiendo cada una de estas variables en senos y cosenos. [1] <br>\n",
    "\n",
    "[1]: https://developer.nvidia.com/blog/three-approaches-to-encoding-time-information-as-features-for-ml-models/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzamos por aplicar one-hot encoding a las variables categoricas no ordinales y guardamoslos resultados en un nuevo dataframe para evitar confusiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar las columnas a codificar\n",
    "cols_to_encode = ['Vlo-I', 'Des-I', 'Emp-I', 'TIPOVUELO', 'OPERA', 'Temporada alta', 'SIGLADES']\n",
    "\n",
    "# Crear un objeto OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Ajustar el codificador a los datos de entrenamiento\n",
    "encoder.fit(df[cols_to_encode])\n",
    "\n",
    "# Codificar las columnas seleccionadas\n",
    "encoded_cols = encoder.transform(df[cols_to_encode]).toarray()\n",
    "\n",
    "# Crear un DataFrame con las columnas codificadas\n",
    "encoded_df = pd.DataFrame(encoded_cols, columns=encoder.get_feature_names_out(cols_to_encode))\n",
    "\n",
    "# Convertir los valores booleanos a enteros\n",
    "encoded_df = encoded_df.astype(int)\n",
    "\n",
    "# Concatenar las columnas codificadas con el DataFrame original\n",
    "new_df = pd.concat([df, encoded_df], axis=1)\n",
    "\n",
    "# Eliminar las columnas originales que fueron codificadas\n",
    "new_df.drop(cols_to_encode, axis=1, inplace=True)\n",
    "\n",
    "# Guardar el codificador en un archivo\n",
    "if not os.path.exists('../encoders'):\n",
    "    os.makedirs('../encoders')\n",
    "with open('../encoders/one-hot-encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(encoder, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora aplicamos label encoding a las variables categoricas ordinales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar las columnas a codificar\n",
    "cols_to_encode = ['Periodo día', 'DIANOM']\n",
    "\n",
    "# Definir las categorías ordinales y su orden\n",
    "periodo_dia_categories = ['mañana', 'tarde', 'noche']\n",
    "dianom_categories = ['Lunes', 'Martes', 'Miercoles', 'Jueves', 'Viernes', 'Sabado', 'Domingo']\n",
    "\n",
    "# Crear un objeto OrdinalEncoder\n",
    "encoder = OrdinalEncoder(categories=[periodo_dia_categories, dianom_categories], dtype=int)\n",
    "\n",
    "# Ajustar el codificador a los datos de entrenamiento\n",
    "encoder.fit(new_df[cols_to_encode])\n",
    "\n",
    "# Codificar las columnas seleccionadas\n",
    "encoded_cols = encoder.transform(new_df[cols_to_encode])\n",
    "\n",
    "# Crear un DataFrame con las columnas codificadas\n",
    "encoded_df = pd.DataFrame(encoded_cols, columns=['Periodo dia_encoded', 'DIANOM_encoded'])\n",
    "\n",
    "# Concatenar las columnas codificadas con el DataFrame original\n",
    "new_df = pd.concat([new_df, encoded_df], axis=1)\n",
    "\n",
    "# Eliminar las columnas originales que fueron codificadas\n",
    "new_df.drop(cols_to_encode, axis=1, inplace=True)\n",
    "\n",
    "# Guardar el codificador en un archivo\n",
    "if not os.path.exists('../encoders'):\n",
    "    os.makedirs('../encoders')\n",
    "with open('../encoders/ordinal-encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(encoder, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora trabajaremos la Fecha-I, para esto primero la descomponemos en mes, dia, hora y minutos. (Estamos ignorando el año ya que todos los datos son del mismo año, a excepcion de dos muestras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la columna Fecha-I a tipo de datos datetime\n",
    "new_df['Fecha-I'] = pd.to_datetime(new_df['Fecha-I'])\n",
    "\n",
    "# Extraer el mes, día, hora y minuto de la columna Fecha-I\n",
    "new_df['Mes'] = new_df['Fecha-I'].dt.month\n",
    "new_df['Día'] = new_df['Fecha-I'].dt.day\n",
    "new_df['Hora'] = new_df['Fecha-I'].dt.hour\n",
    "new_df['Minuto'] = new_df['Fecha-I'].dt.minute\n",
    "\n",
    "# Eliminar la columna original Fecha-I\n",
    "new_df.drop('Fecha-I', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos las variables descompuestas, las codificamos como variables ciclicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(data, col, max_val):\n",
    "    \"\"\"Codifica la columna dada utilizando las funciones seno y coseno.\n",
    "\n",
    "    Args:\n",
    "        data: El DataFrame que se va a codificar.\n",
    "        col: El nombre de la columna que se va a codificar.\n",
    "        max_val: El valor máximo de la columna.\n",
    "\n",
    "    Returns:\n",
    "        El DataFrame con la columna codificada.\n",
    "    \"\"\"\n",
    "\n",
    "    # Primero, calculamos el seno y coseno de los valores de la columna.\n",
    "\n",
    "    data[col + '_sin'] = np.sin(2 * np.pi * data[col] / max_val)\n",
    "    data[col + '_cos'] = np.cos(2 * np.pi * data[col] / max_val)\n",
    "\n",
    "    # Finalmente, retornamos el DataFrame con la columna codificada.\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificar las columnas Month, Day, Hour y Minute\n",
    "new_df = encode(new_df, 'Mes', 12)\n",
    "new_df = encode(new_df, 'Día', 31)\n",
    "new_df = encode(new_df, 'Hora', 24)\n",
    "new_df = encode(new_df, 'Minuto', 60)\n",
    "\n",
    "# Eliminar las columnas originales Mes, Día, Hora y Minuto\n",
    "new_df.drop(['Mes', 'Día', 'Hora', 'Minuto'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set de entrenamineto y prueba\n",
    "\n",
    "Ahora que todas las variables han sido codificadas, separaremos los datos en un set de entrenamiento y un set de prueba, ya que sera importante aislar el set de prueba de las transformaciones que se haran a continuacion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma de X_train: (54564, 777)\n",
      "Forma de y_train: (54564,)\n",
      "Forma de X_test: (13641, 777)\n",
      "Forma de y_test: (13641,)\n"
     ]
    }
   ],
   "source": [
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(new_df.drop('Label', axis=1), new_df['Label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Imprimir las dimensiones de los conjuntos de entrenamiento y prueba\n",
    "print('Forma de X_train:', X_train.shape)\n",
    "print('Forma de y_train:', y_train.shape)\n",
    "print('Forma de X_test:', X_test.shape)\n",
    "print('Forma de y_test:', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos los datos separados, abordaremos el desbalance de clases, ya que hay mas vuelos atrasados que vuelos no atrasados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recuento de etiquetas en el conjunto de entrenamiento:\n",
      "Label\n",
      "1    36336\n",
      "0    18228\n",
      "Name: count, dtype: int64\n",
      "Porcentajes de etiquetas en el conjunto de entrenamiento:\n",
      "Label\n",
      "1    66.593358\n",
      "0    33.406642\n",
      "Name: count, dtype: float64\n",
      "Recuento de etiquetas en el conjunto de prueba:\n",
      "Label\n",
      "1    9170\n",
      "0    4471\n",
      "Name: count, dtype: int64\n",
      "Porcentajes de etiquetas en el conjunto de prueba:\n",
      "Label\n",
      "1    67.223811\n",
      "0    32.776189\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Verificar el equilibrio de las etiquetas en el conjunto de entrenamiento\n",
    "train_counts = y_train.value_counts()\n",
    "train_pct = train_counts / len(y_train) * 100\n",
    "print('Recuento de etiquetas en el conjunto de entrenamiento:')\n",
    "print(train_counts)\n",
    "print('Porcentajes de etiquetas en el conjunto de entrenamiento:')\n",
    "print(train_pct)\n",
    "\n",
    "# Verificar el equilibrio de las etiquetas en el conjunto de prueba\n",
    "test_counts = y_test.value_counts()\n",
    "test_pct = test_counts / len(y_test) * 100\n",
    "print('Recuento de etiquetas en el conjunto de prueba:')\n",
    "print(test_counts)\n",
    "print('Porcentajes de etiquetas en el conjunto de prueba:')\n",
    "print(test_pct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de continuar guardaremos los datos para poderhacer pruebas con y sin el desbalance de clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the training set to a CSV file\n",
    "X_train.to_csv('../data/X_train_desbalanceado.csv', index=False)\n",
    "y_train.to_csv('../data/y_train_desbalanceado.csv', index=False)\n",
    "\n",
    "# Save the test set to a CSV file\n",
    "X_test.to_csv('../data/X_test_desbalanceado.csv', index=False)\n",
    "y_test.to_csv('../data/y_test_desbalanceado.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que confirmamos el debalance de clases en ambos sets de datos, vamos a tomar los vuelos atrasados en entrenamiento y los moveremos al set de datos de prueba, demanera que el set de entrenamiento sea balanceado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontrar la etiqueta de la clase mayoritaria\n",
    "majority_label = train_counts.idxmax()\n",
    "\n",
    "# Encontrar el número de muestras a mover al conjunto de prueba\n",
    "num_to_move = train_counts[majority_label] - train_counts.min()\n",
    "\n",
    "# Seleccionar las muestras excedentes con la clase mayoritaria en el conjunto de entrenamiento\n",
    "excess_samples = X_train[y_train == majority_label].iloc[:num_to_move]\n",
    "\n",
    "# Mover las muestras excedentes al conjunto de prueba\n",
    "X_test = pd.concat([X_test, excess_samples])\n",
    "y_test = pd.concat([y_test, y_train[y_train == majority_label].iloc[:num_to_move]])\n",
    "\n",
    "# Eliminar las muestras excedentes del conjunto de entrenamiento\n",
    "X_train = X_train.drop(excess_samples.index)\n",
    "y_train = y_train.drop(excess_samples.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recuento de etiquetas en el conjunto de entrenamiento:\n",
      "Label\n",
      "0    18228\n",
      "1    18228\n",
      "Name: count, dtype: int64\n",
      "Porcentajes de etiquetas en el conjunto de entrenamiento:\n",
      "Label\n",
      "0    50.0\n",
      "1    50.0\n",
      "Name: count, dtype: float64\n",
      "Recuento de etiquetas en el conjunto de prueba:\n",
      "Label\n",
      "1    27278\n",
      "0     4471\n",
      "Name: count, dtype: int64\n",
      "Porcentajes de etiquetas en el conjunto de prueba:\n",
      "Label\n",
      "1    85.917667\n",
      "0    14.082333\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Verificar el equilibrio de las etiquetas en el conjunto de entrenamiento\n",
    "train_counts = y_train.value_counts()\n",
    "train_pct = train_counts / len(y_train) * 100\n",
    "print('Recuento de etiquetas en el conjunto de entrenamiento:')\n",
    "print(train_counts)\n",
    "print('Porcentajes de etiquetas en el conjunto de entrenamiento:')\n",
    "print(train_pct)\n",
    "\n",
    "# Verificar el equilibrio de las etiquetas en el conjunto de prueba\n",
    "test_counts = y_test.value_counts()\n",
    "test_pct = test_counts / len(y_test) * 100\n",
    "print('Recuento de etiquetas en el conjunto de prueba:')\n",
    "print(test_counts)\n",
    "print('Porcentajes de etiquetas en el conjunto de prueba:')\n",
    "print(test_pct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the training set to a CSV file\n",
    "X_train.to_csv('../data/X_train_balanceado.csv', index=False)\n",
    "y_train.to_csv('../data/y_train_balanceado.csv', index=False)\n",
    "\n",
    "# Save the test set to a CSV file\n",
    "X_test.to_csv('../data/X_test_balanceado.csv', index=False)\n",
    "y_test.to_csv('../data/y_test_balanceado.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
