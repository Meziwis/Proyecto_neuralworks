# Docker Deployment

This folder contains the necessary files for deploying the TensorFlow model using Docker and TensorFlow Serving. The deployment setup allows you to serve the serialized model and monitor its performance using Prometheus.

## Folder Structure

- **Dockerfile**: This file defines the Docker image configuration. It specifies the base image and sets up the necessary environment for serving the TensorFlow model.

- **models**: This folder contains the serialized TensorFlow model and the configuration file for Prometheus monitoring.

  - **bm**: This subfolder represents the model version (e.g., `1`). It contains the model artifacts generated by TensorFlow SavedModel format.

    - **assets**: Contains any additional assets associated with the model.

    - **fingerprint.pb**: The fingerprint file for the model.

    - **saved_model.pb**: The TensorFlow SavedModel file containing the model graph and weights.

    - **variables**: Contains the model's variable values.

      - **variables.data-00000-of-00001**: The variable data file.

      - **variables.index**: The variable index file.

  - **monitoring.config**: The configuration file for Prometheus monitoring. It specifies the metrics and monitoring endpoints for the model server.
- **tf_create_server.sh**: This script file provides the commands to build the Docker image and create a Kubernetes namespace for serving the TensorFlow model. It uses the Dockerfile and tf-serving.yaml for the deployment.
- **tf-serving.yaml**: The YAML configuration file for deploying TensorFlow Serving as a Kubernetes deployment and service. It specifies the deployment details, container image, and service settings.


## Usage

To deploy the TensorFlow model using Docker, follow these steps:

1. Make the tf_create_server.sh script executable:

   ```bash
   chmod +x tf_create_server.sh
   ```
2. If you are using  Minikube in your local machine, you have to give the Docker daemon access to the Minikube Docker engine. To do so, run the following command (optional):

   ```bash
   eval $(minikube docker-env)
   ```

3. Run the script to build the Docker image and create a Kubernetes namespace for serving the model:

   ```bash
    ./tf_create_server.sh
    ```
4. Check the status of the deployment:
   ```bash
   kubectl get pods -n tf-serving
   ```
    The output should be similar to the following:
    ```bash
    NAME                            READY   STATUS    RESTARTS   AGE
    tf-serving-6b7f9b7f5f-4q9q2     1/1     Running   0          2m
    ```
5. Check the logs of the pod to make sure the model is loaded successfully.
    ```bash
    kubectl logs tf-serving-6b7f9b7f5f-4q9q2 -n tf-serving
    ```

6. If you are deploying locally you need to fordward the port to access the service (optional):
    ```bash
    kubectl port-forward service/tf-serving-service 8501:8501 -n tf-serving
    ```

This script will create a Kubernetes namespace for serving the TensorFlow model based on the tf-serving.yaml configuration file.

Note: Ensure that you have Docker and Kubernetes installed and properly configured before running the script.

## Additional Notes

- Ensure that you have Docker installed and running on your system.

- The `Dockerfile` is configured to use the `tensorflow/serving` base image. Make sure the base image and its version are appropriate for your project.

- The `models` folder should contain the serialized TensorFlow model in the expected structure.

- The `tf-serving.yaml` file contains the Kubernetes configuration for deploying TensorFlow Serving as a deployment and service. Modify it according to your specific deployment requirements.

- For more information on TensorFlow Serving, Docker, and Kubernetes, please refer to the official documentation:

- TensorFlow Serving: https://www.tensorflow.org/tfx/guide/serving

- Docker: https://www.docker.com/

- Kubernetes: https://kubernetes.io/

